{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still working with the month of January 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          timestamp  value  node\n",
      "211842    2021-01-20 23:36:20+00:00    440     0\n",
      "211843    2021-01-20 23:36:40+00:00    420     0\n",
      "211844    2021-01-20 23:37:00+00:00    440     0\n",
      "211845    2021-01-20 23:37:20+00:00    440     0\n",
      "211846    2021-01-20 23:37:40+00:00    440     0\n",
      "...                             ...    ...   ...\n",
      "112158837 2021-01-31 22:04:00+00:00    440     0\n",
      "112158838 2021-01-31 22:04:20+00:00    440     0\n",
      "112158839 2021-01-31 22:04:40+00:00    440     0\n",
      "112158840 2021-01-31 22:05:00+00:00    440     0\n",
      "112158841 2021-01-31 22:05:20+00:00    440     0\n",
      "\n",
      "[114586 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "path_P = \"../21-01/year_month=21-01/plugin=ipmi_pub/metric=total_power/a_0.parquet\"\n",
    "dataset_P = pd.read_parquet(path_P, engine='pyarrow')\n",
    "\n",
    "# Casting of node values into integers\n",
    "dataset_P['node'] = dataset_P['node'].astype(int)\n",
    "\n",
    "# I want to extract from the whole dataset only the rows related to the node 0\n",
    "dataset_P_node0 = dataset_P[dataset_P['node'] == 0]\n",
    "del(dataset_P)\n",
    "print(dataset_P_node0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         timestamp  value  node\n",
      "55240462 2021-01-01 00:00:00+00:00    380     0\n",
      "55240463 2021-01-01 00:00:20+00:00    360     0\n",
      "55240464 2021-01-01 00:00:40+00:00    360     0\n",
      "55240465 2021-01-01 00:01:00+00:00    360     0\n",
      "55240466 2021-01-01 00:01:20+00:00    360     0\n",
      "...                            ...    ...   ...\n",
      "69375705 2021-01-31 23:30:00+00:00    440     0\n",
      "69375706 2021-01-31 23:30:20+00:00    420     0\n",
      "69375707 2021-01-31 23:30:40+00:00    440     0\n",
      "69375708 2021-01-31 23:31:00+00:00    420     0\n",
      "69375709 2021-01-31 23:43:40+00:00    440     0\n",
      "\n",
      "[114586 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# As usual I need to order by datetime the samples I have\n",
    "dataset_P_node0['timestamp'] = pd.to_datetime(dataset_P_node0['timestamp'])\n",
    "\n",
    "dataset_P_node0 = dataset_P_node0.sort_values(by='timestamp')\n",
    "\n",
    "print(dataset_P_node0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               datetime  power\n",
      "0   2021-01-01 00:00:00  65920\n",
      "1   2021-01-01 01:00:00  66480\n",
      "2   2021-01-01 02:00:00  66300\n",
      "3   2021-01-01 03:00:00  66220\n",
      "4   2021-01-01 04:00:00  66560\n",
      "..                  ...    ...\n",
      "686 2021-01-31 19:00:00  12160\n",
      "687 2021-01-31 20:00:00   7820\n",
      "688 2021-01-31 21:00:00  20500\n",
      "689 2021-01-31 22:00:00   7420\n",
      "690 2021-01-31 23:00:00   6020\n",
      "\n",
      "[691 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now we need to sum the values in order to obtain a cumulative single value for each hour\n",
    "dataset_P_node0['date_hour'] = dataset_P_node0['timestamp'].dt.strftime('%Y-%m-%d %H')\n",
    "hourly_sum_P = dataset_P_node0.groupby('date_hour')['value'].sum()\n",
    "\n",
    "# And here we convert the pandas series into a dataframe with the corresponding datetimes\n",
    "dataset_P_node0 = pd.DataFrame({'datetime': pd.to_datetime(hourly_sum_P.index, format='%Y-%m-%d %H'), 'power': hourly_sum_P.values})\n",
    "del(hourly_sum_P)\n",
    "\n",
    "print(dataset_P_node0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               datetime  power\n",
      "0   2021-01-01 00:00:00  65920\n",
      "1   2021-01-01 01:00:00  66480\n",
      "2   2021-01-01 02:00:00  66300\n",
      "3   2021-01-01 03:00:00  66220\n",
      "4   2021-01-01 04:00:00  66560\n",
      "..                  ...    ...\n",
      "739 2021-01-31 19:00:00  12160\n",
      "740 2021-01-31 20:00:00   7820\n",
      "741 2021-01-31 21:00:00  20500\n",
      "742 2021-01-31 22:00:00   7420\n",
      "743 2021-01-31 23:00:00   6020\n",
      "\n",
      "[744 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Since some hours are missing in the dataframe we need to add them with a null power value\n",
    "start_datetime = pd.Timestamp('2021-01-01 00:00:00')\n",
    "end_datetime = pd.Timestamp('2021-01-31 23:00:00')\n",
    "expected_datetimes = pd.date_range(start=start_datetime, end=end_datetime, freq='h')\n",
    "\n",
    "missing_datetimes = expected_datetimes[~expected_datetimes.isin(dataset_P_node0['datetime'])]\n",
    "missing_data = pd.DataFrame({'datetime': missing_datetimes, 'power': 0})\n",
    "\n",
    "dataset_P_node0 = pd.concat([dataset_P_node0, missing_data]).sort_values(by='datetime').reset_index(drop=True)\n",
    "print(dataset_P_node0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               datetime  Carbon Intensity gCO₂eq/kWh (direct)\n",
      "0   2021-01-01 00:00:00                                303.29\n",
      "1   2021-01-01 01:00:00                                303.45\n",
      "2   2021-01-01 02:00:00                                295.04\n",
      "3   2021-01-01 03:00:00                                295.56\n",
      "4   2021-01-01 04:00:00                                308.06\n",
      "..                  ...                                   ...\n",
      "739 2021-01-31 19:00:00                                244.04\n",
      "740 2021-01-31 20:00:00                                253.52\n",
      "741 2021-01-31 21:00:00                                263.28\n",
      "742 2021-01-31 22:00:00                                262.85\n",
      "743 2021-01-31 23:00:00                                266.70\n",
      "\n",
      "[744 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our power dataset formatted as needed, it's the time to extract also the carbon intensity dataset\n",
    "path_CI = \"../IT-NO_2021_hourly.csv\"\n",
    "dataset_CI = pd.read_csv(path_CI)\n",
    "dataset_CI = dataset_CI.drop(columns=['Carbon Intensity gCO₂eq/kWh (LCA)', 'Country', 'Low Carbon Percentage' ,'Renewable Percentage' ,'Zone Name', 'Zone Id', 'Data Source', 'Data Estimated', 'Data Estimation Method'])\n",
    "\n",
    "dataset_CI['Carbon Intensity gCO₂eq/kWh (direct)'] = dataset_CI['Carbon Intensity gCO₂eq/kWh (direct)'].fillna(dataset_CI.describe(include='all').loc['mean'].loc['Carbon Intensity gCO₂eq/kWh (direct)'])\n",
    "\n",
    "dataset_CI['Datetime (UTC)'] = pd.to_datetime(dataset_CI['Datetime (UTC)'])\n",
    "\n",
    "end_of_january = pd.to_datetime('2021-02-01')\n",
    "dataset_CI = dataset_CI[dataset_CI['Datetime (UTC)'] < end_of_january]\n",
    "\n",
    "dataset_CI = dataset_CI.rename(columns={'Datetime (UTC)': 'datetime'})\n",
    "\n",
    "print(dataset_CI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "Cannot aggregate non-numeric type: datetime64[ns]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\loren\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\window\\rolling.py:489\u001b[0m, in \u001b[0;36mBaseWindow._apply_columnwise\u001b[1;34m(self, homogeneous_func, name, numeric_only)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\loren\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\window\\rolling.py:361\u001b[0m, in \u001b[0;36mBaseWindow._prep_values\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_i8_conversion(values\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mops for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# GH #12373 : rolling functions error on float32 data\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# make sure the data is coerced to float64\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ops for Rolling for this dtype datetime64[ns] are not implemented",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We'll use window sizes of 6, 12 and 24 hours, 1 week and also 1 month and compare the results\u001b[39;00m\n\u001b[0;32m      2\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;66;03m# hours\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dataset_P_MA \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_P_node0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_P_MA)\n",
      "File \u001b[1;32mc:\\Users\\loren\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\window\\rolling.py:2259\u001b[0m, in \u001b[0;36mRolling.mean\u001b[1;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   2216\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   2217\u001b[0m     template_header,\n\u001b[0;32m   2218\u001b[0m     create_section_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2257\u001b[0m     engine_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2258\u001b[0m ):\n\u001b[1;32m-> 2259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loren\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\window\\rolling.py:1625\u001b[0m, in \u001b[0;36mRollingAndExpandingMixin.mean\u001b[1;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   1623\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_apply(sliding_mean, engine_kwargs)\n\u001b[0;32m   1624\u001b[0m window_func \u001b[38;5;241m=\u001b[39m window_aggregations\u001b[38;5;241m.\u001b[39mroll_mean\n\u001b[1;32m-> 1625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loren\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\window\\rolling.py:619\u001b[0m, in \u001b[0;36mBaseWindow._apply\u001b[1;34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_columnwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "File \u001b[1;32mc:\\Users\\loren\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\window\\rolling.py:491\u001b[0m, in \u001b[0;36mBaseWindow._apply_columnwise\u001b[1;34m(self, homogeneous_func, name, numeric_only)\u001b[0m\n\u001b[0;32m    489\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_values(arr)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot aggregate non-numeric type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    493\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    494\u001b[0m res \u001b[38;5;241m=\u001b[39m homogeneous_func(arr)\n\u001b[0;32m    495\u001b[0m res_values\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "\u001b[1;31mDataError\u001b[0m: Cannot aggregate non-numeric type: datetime64[ns]"
     ]
    }
   ],
   "source": [
    "# We'll use window sizes of 6, 12 and 24 hours, 1 week and also 1 month and compare the results\n",
    "window_size = 6 # hours\n",
    "dataset_P_MA = dataset_P_node0.rolling(window=window_size).mean()\n",
    "\n",
    "dataset_CI_MA = dataset_CI.rolling(window=window_size).mean()\n",
    "\n",
    "print(dataset_P_MA)\n",
    "print(dataset_CI_MA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
